{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lime\n",
    "\n",
    "from lime import lime_tabular\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Example Using a Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = pd.read_csv('../data/generated_data_numpy.csv')\n",
    "\n",
    "original_data = fake_data.copy()\n",
    "fake_data = fake_data.drop(columns=['Policy_Id', 'Policy_Year', 'Policy_Month'])\n",
    "fake_data['Accident_Reported'] = np.where(fake_data['Accident_Reported'] == 1, 'Accident', 'No Accident')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, labels_train, labels_test = sklearn.model_selection.train_test_split(\n",
    "    fake_data[[x for x in fake_data.columns if x != 'Accident_Reported']],\n",
    "    fake_data['Accident_Reported'],\n",
    "    train_size=0.80)\n",
    "\n",
    "train = pd.DataFrame(train)\n",
    "test = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of categorical features\n",
    "categorical_features = ['Make', 'Body_Style', 'Model_Color', 'Driver_Hair_Color']\n",
    "\n",
    "# get a list of numeric features\n",
    "numeric_features = [x for x in fake_data.columns if x not in categorical_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get categorical features idx\n",
    "categorical_features_idx = list(np.where(np.isin(train.columns, categorical_features))[0])\n",
    "\n",
    "# get numeric features idx\n",
    "numeric_features_idx = list(np.where(np.isin(train.columns, numeric_features))[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(categorical_features_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to convert text to numbers for LimeExplainer to work unfortunately..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features using LabelEncoder\n",
    "label_encoders = {}\n",
    "for feature in categorical_features:\n",
    "    label_encoders[feature] = LabelEncoder()\n",
    "    train[feature] = label_encoders[feature].fit_transform(train[feature])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the label encoders to encode the test data\n",
    "for feature in categorical_features:\n",
    "    test[feature] = label_encoders[feature].transform(test[feature])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and store the label encoded values so that we can have the proper names in the lime explainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_values = {}\n",
    "for feature in categorical_features:\n",
    "    # get column number\n",
    "    col_num = train.columns.get_loc(feature)\n",
    "    categorical_values[col_num] = list(label_encoders[feature].classes_)\n",
    "\n",
    "categorical_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a quick pipeline model to use for evaluating the method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a numeric transformer \n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())], \n",
    "    verbose=True)\n",
    "\n",
    "# build a categorical transformer with simple imputer and one hot encoder\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))], \n",
    "    verbose=True)\n",
    "\n",
    "# build a preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features_idx),\n",
    "        ('cat', categorical_transformer, categorical_features_idx)])\n",
    "\n",
    "# build a classifier\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# build a pipeline to apply the preprocessing and the classifier\n",
    "pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                       ('classifier', clf)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the pipeline\n",
    "pipe.fit(train.to_numpy(), labels_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the pipeline on the test set\n",
    "roc_auc_score(labels_test, pipe.predict_proba(test.to_numpy())[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate accuracy of the pipeline on the test set\n",
    "pipe.score(test.to_numpy(), labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an explainer for the pipeline\n",
    "explainer = lime_tabular.LimeTabularExplainer(train.values,\n",
    "                                              feature_names=train.columns,\n",
    "                                              categorical_features=categorical_features_idx,\n",
    "                                              categorical_names=categorical_values,\n",
    "                                              mode='classification',\n",
    "                                              class_names=clf.classes_,\n",
    "                                              discretize_continuous=True,\n",
    "                                              sample_around_instance=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peek(x):        \n",
    "    test_instance = test.iloc[x].copy()\n",
    "    for feature in categorical_features:\n",
    "        test_instance[feature] = label_encoders[feature].inverse_transform([test_instance[feature]])[0]\n",
    "    # add labels_test to test_instance\n",
    "    test_instance['Risk'] = labels_test.iloc[x]\n",
    "    return test_instance\n",
    "\n",
    "test_num = 0\n",
    "peek(test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain the first instance in the test set\n",
    "exp = explainer.explain_instance(test.values[test_num], pipe.predict_proba, num_features=7)\n",
    "exp.show_in_notebook(show_table=True, show_all=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# calculate the \"actual\" probability based on these features\n",
    "print(f\"Actual: {sigmoid((17685-10000)/5000)}\")\n",
    "\n",
    "# calculuate our local interpretable model's probability\n",
    "print(f\"Local: {sigmoid(np.exp(0.1)*np.exp(0.2))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_labels, pertrubed_data = explainer._LimeTabularExplainer__data_inverse(test.values[test_num], 1000)\n",
    "perturbed = pd.DataFrame(pertrubed_data, columns=test.columns)\n",
    "perturbed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying another instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_num = 1000\n",
    "accident_test = test.iloc[test_num].values\n",
    "peek(test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store an example for exploration\n",
    "example = test[test_num:(test_num + 1)]\n",
    "\n",
    "# predict the example\n",
    "reported_prob = lambda x: pipe.predict_proba(x.to_numpy())[0][1]\n",
    "\n",
    "# verify we get the same probablity as above\n",
    "reported_prob(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(example.values[0], pipe.predict_proba, num_features=7)\n",
    "exp.show_in_notebook(show_table=True, show_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try perturbing this a bit and see how it changes. Based on the above, let's try adjusting the Years_Customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impact = {}\n",
    "for years in range(1, 15):\n",
    "    example.loc[:, \"Years_Customer\"] = years\n",
    "    impact[years] = reported_prob(example)\n",
    "\n",
    "pd.DataFrame(impact, index=[\"Reported Probability\"]).T.plot(legend=False, \n",
    "                                                            title=\"Impact of Years as Customer on Reported Probability\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_labels, pertrubed_data = explainer._LimeTabularExplainer__data_inverse(test.values[test_num], 10000)\n",
    "perturbed = pd.DataFrame(pertrubed_data, columns=test.columns)\n",
    "perturbed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed = pd.DataFrame(pertrubed_data, columns=test.columns)\n",
    "perturbed_predictions = pipe.predict_proba(perturbed)\n",
    "perturbed_predictions = perturbed_predictions[:, 1]\n",
    "perturbed[\"Risk\"] = perturbed_predictions\n",
    "\n",
    "# plot years customer by risk\n",
    "perturbed.plot.scatter(x=\"Years_Customer\", y=\"Risk\", title=\"Impact of Years as Customer on Reported Probability\")\n",
    "# add line of best fit\n",
    "plt.plot(np.unique(perturbed[\"Years_Customer\"]), np.poly1d(np.polyfit(perturbed[\"Years_Customer\"], perturbed[\"Risk\"], 1))(np.unique(perturbed[\"Years_Customer\"])), color='red')\n",
    "# add text with coefficient\n",
    "plt.text(10, 0.5, f\"Coefficient: {np.polyfit(perturbed['Years_Customer'], perturbed['Risk'], 1)[0]:.2f}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Everything below here is just getting explanations for the whole test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we generate predictions along with our explanations of those predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipe.predict_proba(test)\n",
    "# # get just the second column\n",
    "predictions = predictions[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_test_data = original_data[original_data.index.isin(test.index)]\n",
    "original_test_data.loc[test.index, 'Predictions'] = predictions\n",
    "original_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_instance(row_number=0):\n",
    "    exp = explainer.explain_instance(test.iloc[row_number], pipe.predict_proba, num_features=7)\n",
    "    el = exp.as_list()\n",
    "    # convert el to a dataframe\n",
    "    el = pd.DataFrame(el, columns=['feature', 'weight'])\n",
    "    if labels_test.iloc[row_number] == 0:\n",
    "        el = el.loc[el.weight < 0]\n",
    "    else:\n",
    "        el = el.loc[el.weight > 0]\n",
    "    el[\"abs_weight\"] = el[\"weight\"].abs()\n",
    "    el = el.sort_values(by='abs_weight', ascending=False)\n",
    "    el[\"Policy_Id\"] = original_test_data.iloc[row_number][\"Policy_Id\"]\n",
    "    return el.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain all instances and store them in a pandas dataframe\n",
    "explanations = []\n",
    "for i in range(0, len(test)):\n",
    "    explanations.append(explain_instance(i))\n",
    "\n",
    "explanations = pd.concat(explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations.to_csv(\"../data/explanations.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
