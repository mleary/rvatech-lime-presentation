{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lime\n",
    "\n",
    "from lime import lime_tabular\n",
    "from IPython.display import Image\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model on the generated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = pd.read_csv('../data/generated_data_numpy.csv')\n",
    "\n",
    "original_data = fake_data.copy()\n",
    "fake_data = fake_data.drop(columns=['Policy_Id', 'Policy_Year', 'Policy_Month'])\n",
    "fake_data['Accident_Reported'] = np.where(fake_data['Accident_Reported'] == 1, 'Accident', 'No Accident')\n",
    "fake_data['Accident_Reported'] = fake_data['Accident_Reported'].astype('category')\n",
    "fake_data['Accident_Reported'] = fake_data['Accident_Reported'].cat.reorder_categories(['Accident', 'No Accident'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, labels_train, labels_test = sklearn.model_selection.train_test_split(\n",
    "    fake_data[[x for x in fake_data.columns if x != 'Accident_Reported']],\n",
    "    fake_data['Accident_Reported'],\n",
    "    train_size=0.80)\n",
    "\n",
    "train = pd.DataFrame(train)\n",
    "test = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of categorical features\n",
    "categorical_features = ['Make', 'Body_Style', 'Model_Color', 'Driver_Hair_Color']\n",
    "\n",
    "# get a list of numeric features\n",
    "numeric_features = [x for x in fake_data.columns if x not in categorical_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get categorical features idx\n",
    "categorical_features_idx = list(np.where(np.isin(train.columns, categorical_features))[0])\n",
    "\n",
    "# get numeric features idx\n",
    "numeric_features_idx = list(np.where(np.isin(train.columns, numeric_features))[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(categorical_features_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to convert text to numbers for LimeExplainer to work unfortunately..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features using LabelEncoder\n",
    "label_encoders = {}\n",
    "for feature in categorical_features:\n",
    "    label_encoders[feature] = LabelEncoder()\n",
    "    train[feature] = label_encoders[feature].fit_transform(train[feature])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the label encoders to encode the test data\n",
    "for feature in categorical_features:\n",
    "    test[feature] = label_encoders[feature].transform(test[feature])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and store the label encoded values so that we can have the proper names in the lime explainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_values = {}\n",
    "for feature in categorical_features:\n",
    "    # get column number\n",
    "    col_num = train.columns.get_loc(feature)\n",
    "    categorical_values[col_num] = list(label_encoders[feature].classes_)\n",
    "\n",
    "categorical_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a quick pipeline model to use for evaluating the method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a numeric transformer \n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())], \n",
    "    verbose=True)\n",
    "\n",
    "# build a categorical transformer with simple imputer and one hot encoder\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))], \n",
    "    verbose=True)\n",
    "\n",
    "# build a preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features_idx),\n",
    "        ('cat', categorical_transformer, categorical_features_idx)])\n",
    "\n",
    "# build a classifier\n",
    "clf = RandomForestClassifier(n_estimators=1000, min_samples_split=40)\n",
    "\n",
    "# build a pipeline to apply the preprocessing and the classifier\n",
    "pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                       ('classifier', clf)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the pipeline\n",
    "pipe.fit(train.to_numpy(), labels_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = pipe.predict(test.to_numpy())\n",
    "# original_test_data = original_data[original_data.index.isin(test.index)]\n",
    "# original_test_data.loc[test.index, 'Predictions'] = predictions\n",
    "# original_test_data.to_csv('../data/predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High-level view (\"Earth-view\" model statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the pipeline on the test set\n",
    "roc_auc_score(np.where(labels_test == 'Accident', 1, 0), pipe.predict_proba(test.to_numpy())[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate accuracy of the pipeline on the test set\n",
    "pipe.score(test.to_numpy(), labels_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for some LIME!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does LIME actually work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../lime_diagram.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an explainer for the pipeline\n",
    "explainer = lime_tabular.LimeTabularExplainer(train.values,\n",
    "                                              feature_names=train.columns,\n",
    "                                              categorical_features=categorical_features_idx,\n",
    "                                              categorical_names=categorical_values,\n",
    "                                              mode='classification',\n",
    "                                              class_names=clf.classes_,\n",
    "                                              discretize_continuous=True,\n",
    "                                              sample_around_instance=True,\n",
    "                                              kernel_width=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peek(x):        \n",
    "    test_instance = test.iloc[x].copy(deep=True)\n",
    "    for feature in categorical_features:\n",
    "        test_instance[feature] = label_encoders[feature].inverse_transform([test_instance[feature]])[0]\n",
    "    # add labels_test to test_instance\n",
    "    print(f\"Actual Outcome: {labels_test.iloc[x]}\")\n",
    "    return test_instance\n",
    "\n",
    "test_num = 4\n",
    "peek(test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain the first instance in the test set\n",
    "exp = explainer.explain_instance(test.values[test_num], pipe.predict_proba, num_features=7)\n",
    "exp.show_in_notebook(show_table=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../rvatech_diagram.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# calculate the \"actual\" probability based on these features\n",
    "print(f\"Actual: {sigmoid((24939-10000)/5000)}\")\n",
    "\n",
    "# calculate the \"global\" probability\n",
    "print(f\"Global: {exp.predict_proba[0]}\")\n",
    "\n",
    "# calculate our local interpretable model's probability\n",
    "print(f\"Local: {exp.local_pred[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the right value for kernel width!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_values = dict()\n",
    "range_values = np.arange(0.01, 7.0, 0.1)\n",
    "\n",
    "for i in range_values:\n",
    "    explainer = lime_tabular.LimeTabularExplainer(train.values,\n",
    "                                                feature_names=train.columns,\n",
    "                                                categorical_features=categorical_features_idx,\n",
    "                                                categorical_names=categorical_values,\n",
    "                                                mode='classification',\n",
    "                                                class_names=clf.classes_,\n",
    "                                                discretize_continuous=True,\n",
    "                                                sample_around_instance=True,\n",
    "                                                kernel_width=i)\n",
    "    exp = explainer.explain_instance(test.values[test_num], pipe.predict_proba, num_features=7)\n",
    "    abs_values[i] = abs(exp.local_pred[0] - exp.predict_proba[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot abs_values\n",
    "plt.plot(list(abs_values.keys()), list(abs_values.values()))\n",
    "# change size of plot\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "plt.xlabel('Kernel Width')\n",
    "plt.ylabel('Difference in Probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime_tabular.LimeTabularExplainer(train.values,\n",
    "                                            feature_names=train.columns,\n",
    "                                            categorical_features=categorical_features_idx,\n",
    "                                            categorical_names=categorical_values,\n",
    "                                            mode='classification',\n",
    "                                            class_names=clf.classes_,\n",
    "                                            discretize_continuous=True,\n",
    "                                            sample_around_instance=True,\n",
    "                                            kernel_width=0.85)\n",
    "exp = explainer.explain_instance(test.values[test_num], pipe.predict_proba, num_features=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(test.values[test_num], pipe.predict_proba, num_features=7)\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../rvatech_diagram.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No strict guarantee of local model approximating global model! In this case, the data-generating process if fairly simple so this worked out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_labels, perturbed_data = explainer._LimeTabularExplainer__data_inverse(test.values[test_num], 100000)\n",
    "perturbed = pd.DataFrame(perturbed_data, columns=test.columns)\n",
    "perturbed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying another instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_num = 9\n",
    "accident_test = test.iloc[test_num].values\n",
    "peek(test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the example\n",
    "reported_prob = lambda x: pipe.predict_proba(x.reshape(1, -1))[0][0]\n",
    "\n",
    "# verify we get the same probability as above\n",
    "reported_prob(accident_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(accident_test, pipe.predict_proba, num_features=7)\n",
    "exp.show_in_notebook(show_table=True, show_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../rvatech_diagram.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try perturbing this a bit and see how it changes. Based on the above, let's try adjusting a variable to see what happens to predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = accident_test.copy()\n",
    "\n",
    "impact = {}\n",
    "for miles in range(0, 20000, 1000):\n",
    "    temp[4] = miles\n",
    "    impact[miles] = reported_prob(temp)\n",
    "\n",
    "pd.DataFrame(impact, index=[\"Reported Probability\"]).T.plot(legend=False, \n",
    "                                                            title=\"Impact of Miles Driven on Reported Probability\")\n",
    "# make chart bigger\n",
    "plt.gcf().set_size_inches(10, 6)\n",
    "# add axis labels\n",
    "plt.xlabel(\"Miles\")\n",
    "plt.ylabel(\"Reported Probability\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impact = {}\n",
    "for miles in range(0, 20000, 1000):\n",
    "    temp[4] = miles\n",
    "    temp[1] = 1\n",
    "    impact[miles] = reported_prob(temp)\n",
    "\n",
    "pd.DataFrame(impact, index=[\"Reported Probability\"]).T.plot(legend=False, \n",
    "                                                            title=\"Impact of Miles Driven on Reported Probability\")\n",
    "# make chart bigger\n",
    "plt.gcf().set_size_inches(10, 6)\n",
    "# add axis labels\n",
    "plt.xlabel(\"Miles\")\n",
    "plt.ylabel(\"Reported Probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_labels, perturbed_data = explainer._LimeTabularExplainer__data_inverse(accident_test, 100000)\n",
    "perturbed = pd.DataFrame(perturbed_data, columns=test.columns)\n",
    "perturbed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed = pd.DataFrame(perturbed_data, columns=test.columns)\n",
    "perturbed_predictions = pipe.predict_proba(perturbed)\n",
    "perturbed_predictions = perturbed_predictions[:, 0]\n",
    "perturbed[\"Risk\"] = perturbed_predictions\n",
    "\n",
    "# highlight dense regions\n",
    "perturbed.plot.hexbin(x=\"Miles_Driven\", y=\"Risk\", gridsize=20, cmap='Oranges')\n",
    "# make chart bigger\n",
    "plt.gcf().set_size_inches(12, 8)\n",
    "# add line of best fit\n",
    "# add axis labels\n",
    "plt.xlabel(\"Miles\")\n",
    "plt.ylabel(\"Reported Probability\")\n",
    "plt.plot(np.unique(perturbed[\"Miles_Driven\"]), \n",
    "         np.poly1d(np.polyfit(perturbed[\"Miles_Driven\"], \n",
    "                              perturbed[\"Risk\"], 1))(np.unique(perturbed[\"Miles_Driven\"])), \n",
    "                              color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Everything below here is just getting explanations for the whole test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we generate predictions along with our explanations of those predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipe.predict_proba(test)\n",
    "# # get just the first column\n",
    "predictions = predictions[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_test_data = original_data[original_data.index.isin(test.index)]\n",
    "original_test_data.loc[test.index, 'Predictions'] = predictions\n",
    "original_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_instance(row_number=0):\n",
    "    exp = explainer.explain_instance(test.iloc[row_number], pipe.predict_proba, num_features=7)\n",
    "    el = exp.as_list()\n",
    "    # convert el to a dataframe\n",
    "    el = pd.DataFrame(el, columns=['feature', 'weight'])\n",
    "    if labels_test.iloc[row_number] == 0:\n",
    "        el = el.loc[el.weight < 0]\n",
    "    else:\n",
    "        el = el.loc[el.weight > 0]\n",
    "    el[\"abs_weight\"] = el[\"weight\"].abs()\n",
    "    el = el.sort_values(by='abs_weight', ascending=False)\n",
    "    el[\"Policy_Id\"] = original_test_data.iloc[row_number][\"Policy_Id\"]\n",
    "    return el.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain all instances and store them in a pandas dataframe\n",
    "explanations = []\n",
    "for i in range(0, len(test)):\n",
    "    explanations.append(explain_instance(i))\n",
    "\n",
    "explanations = pd.concat(explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations.to_csv(\"../data/explanations.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
